{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike-Share Demand Forecasting 1: Data Preparation\n",
    "\n",
    "In this timeseries forecasting example we investigate demand in the [Capital Bikeshare scheme in 2011-12](https://www.capitalbikeshare.com/system-data), with relation to weather data.\n",
    "\n",
    "This notebook downloads the [original weather-annotated dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) from the UC Irvine website, and performs some basic preparations before uploading to S3.\n",
    "\n",
    "Later notebooks in this series fit models to the uploaded prepared data, and compare their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and configuration<a class=\"anchor\" id=\"setup\"/>\n",
    "\n",
    "As usual we start by loading libraries, defining configuration, and connecting to AWS SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = # TODO: Choose a bucket you've created, and SageMaker has full access to\n",
    "%store bucket\n",
    "\n",
    "# Data files to be stored in S3:\n",
    "data_prefix = \"data/\"\n",
    "%store data_prefix\n",
    "target_train_filename = \"target_train.csv\"\n",
    "%store target_train_filename\n",
    "target_test_filename = \"target_test.csv\"\n",
    "%store target_test_filename\n",
    "related_filename = \"related.csv\"\n",
    "%store related_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to our AWS SDKs, and initialise our access role (which may wait a little while to ensure any newly created permissions propagate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "s3 = session.client(service_name=\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch the source data<a class=\"anchor\" id=\"fetch\"/>\n",
    "\n",
    "Since this data set is comparatively small, we can process it here in the notebook instance without requiring large disk allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.zip http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "!rm -rf ./data/raw\n",
    "!mkdir -p ./data/raw\n",
    "!unzip data.zip -d ./data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UCI dataset documentation](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) tells us what to expect in terms of column headers, ranges and types, which we can load here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\n",
    "    \"./data/raw/hour.csv\",\n",
    "    index_col=\"instant\",\n",
    "    dtype={\n",
    "        \"dteday\": str,\n",
    "        \"season\": int,\n",
    "        \"yr\": int,\n",
    "        \"mnth\": int,\n",
    "        \"hr\": int,\n",
    "        \"holiday\": bool,\n",
    "        \"weekday\": int,\n",
    "        \"workingday\": bool,\n",
    "        \"weathersit\": int,\n",
    "        \"temp\": float,\n",
    "        \"atemp\": float,\n",
    "        \"hum\": float,\n",
    "        \"windspeed\": float,\n",
    "        \"casual\": int,\n",
    "        \"registered\": int,\n",
    "        \"cnt\": int\n",
    "    }\n",
    ").sort_values([\"dteday\", \"hr\"])\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but we also have a quick check of the ranges and variation to check our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load data & interpolate gaps<a class=\"anchor\" id=\"convert\"/>\n",
    "\n",
    "The raw dataset has:\n",
    "\n",
    "* Some date/time column redundancy which we'll cut down\n",
    "* No rows present for hours where total demand was zero (see above summary)\n",
    "\n",
    "...which we'll deal with here, starting with the timestamp tidy-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = raw_df.copy()\n",
    "\n",
    "# Combine day and hour into datetime column, and drop superfluous date features:\n",
    "hourly_df[\"dteday\"] = pd.to_datetime(raw_df[\"dteday\"].map(str) + \" \" + raw_df[\"hr\"].map(str) + \":00\")\n",
    "hourly_df = hourly_df.rename(columns={ \"dteday\": \"timestamp\" }).drop(columns=[\"yr\", \"mnth\", \"hr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are missing rows for hours where nobody used bikes, there's at least one row for every day in the covered range.\n",
    "\n",
    "Therefore we can fill in gaps pretty reliably as follows, for the benefit of any forecast methods that can't handle missing data:\n",
    "\n",
    "1. Rider counts (casual, registered, cnt) are set to 0 (cnt=0 can be used to recover to which rows imputation was applied)\n",
    "2. Daily values (season, holiday, weekday, workingday) are the mean of source records from that day (not that the aggregation should matter, because they sohuld all match)\n",
    "3. Weather data (weathersit, temp, atemp, hum, windspeed) are interpolated over time from nearest present records.\n",
    "\n",
    "Of these the weather data is the only real imputation, and time-wise interpolation should be a pretty solid approach to fill in the occasional missing hour of hourly weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the full range of hours covered by the data-set, to the number of records:\n",
    "daterange = pd.date_range(\n",
    "    start=list(hourly_df[\"timestamp\"])[0],\n",
    "    end=list(hourly_df[\"timestamp\"])[-1],\n",
    "    freq='H'\n",
    ")\n",
    "\n",
    "n_raw_records = len(hourly_df)\n",
    "n_range_hours = len(daterange)\n",
    "print(f\"{n_raw_records} raw records vs {n_range_hours} hours in date range\")\n",
    "if (n_raw_records == n_range_hours):\n",
    "    print(\"Data fully specified\")\n",
    "elif (n_raw_records < n_range_hours):\n",
    "    # (We expect to see this)\n",
    "    print(\"MISMATCH: Missing data will be interpolated\")\n",
    "elif (n_raw_records > n_range_hours):\n",
    "    raise NotImplementedError(\"This script can't deal with duplicates yet!\")\n",
    "\n",
    "# Construct a fully range-covering table\n",
    "# (including day-granularity fields taken from aggregating the source table)\n",
    "tmp = pd.DataFrame({ \"timestamp\": daterange })\n",
    "tmp[\"dteday\"] = tmp[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "fill_df = tmp.merge(\n",
    "    raw_df.groupby(\"dteday\").agg(\"mean\")[[\"season\", \"holiday\", \"weekday\", \"workingday\"]],\n",
    "    how=\"left\",\n",
    "    on=\"dteday\"\n",
    ").drop(columns=[\"dteday\"])\n",
    "\n",
    "# Join the whole-range table to our target, and fill in the day-granularity fields\n",
    "assert (\n",
    "    hourly_df.isna().sum().sum() == 0\n",
    "), \"These imputations assume no missing values in source data set records!\"\n",
    "imputed_df = fill_df.merge(hourly_df, how=\"left\", on=\"timestamp\", suffixes=(\"_day\", \"\"))\n",
    "imputed_df[\"season\"].fillna(imputed_df[\"season_day\"], inplace=True)\n",
    "imputed_df[\"holiday\"].fillna(imputed_df[\"holiday_day\"], inplace=True)\n",
    "imputed_df[\"weekday\"].fillna(imputed_df[\"weekday_day\"], inplace=True)\n",
    "imputed_df[\"workingday\"].fillna(imputed_df[\"workingday_day\"], inplace=True)\n",
    "imputed_df.drop(columns=[\"season_day\", \"holiday_day\", \"weekday_day\", \"workingday_day\"], inplace=True)\n",
    "\n",
    "# Fill all missing demand values with zero (which is why the records were missing in the first place)\n",
    "imputed_df[\"casual\"].fillna(0, inplace=True)\n",
    "imputed_df[\"registered\"].fillna(0, inplace=True)\n",
    "imputed_df[\"cnt\"].fillna(0, inplace=True)\n",
    "\n",
    "# Interpolate over time for missing weather data fields:\n",
    "imputed_df = imputed_df.set_index(\"timestamp\")\n",
    "imputed_df[\"weathersit\"] = imputed_df[\"weathersit\"].interpolate(method=\"time\").round()\n",
    "imputed_df[\"temp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"atemp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"hum\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"windspeed\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df = imputed_df.reset_index()\n",
    "\n",
    "assert not imputed_df.isna().any().any(), \"Imputed DF should not have any remaining nulls!\"\n",
    "assert len(imputed_df) == n_range_hours, \"Imputed DF should fully cover time range!\"\n",
    "\n",
    "print(\"Imputation complete\")\n",
    "imputed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to extend feature engineering here, just leaving timestamp and *_demand columns alone:\n",
    "target_suffix = \"_demand\"\n",
    "full_df = imputed_df.rename(columns={\n",
    "    \"casual\": f\"casual{target_suffix}\",\n",
    "    \"registered\": f\"registered{target_suffix}\",\n",
    "    \"cnt\": f\"total{target_suffix}\"\n",
    "})\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data<a class=\"anchor\" id=\"explore\"/>\n",
    "\n",
    "Some basic visualisations will be helpful to understand the overall structure of the timeseries.\n",
    "\n",
    "First we will plot the overall demand across the lifetime of the full data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "full_df.plot(x=\"timestamp\", y=\"registered_demand\", ax=ax, label=\"Registered Customers\")\n",
    "full_df.plot(x=\"timestamp\", y=\"casual_demand\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - Whole Data Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we see from the above that:\n",
    "\n",
    "1. \"registered\" or non-casual customers source the majority of demand\n",
    "2. Although summer/winter seasonality can be seen in the data, the overall growth / popularity increase of the service between the two years is also significant\n",
    "3. There's strong periodicity/\"spikiness\" at at shorter time periods than can be seen on the overall data view\n",
    "\n",
    "**Note: Point 2 is challenging for the accuracy of our forecasts because:**\n",
    "\n",
    "* We only have 2 years of data, so if annual seasonality looks significant we're in trouble because in general we'd like to have history covering **~5x the longest cycle time** for significant patterns\n",
    "* The overall popularity of the service looks significantly **non-stationary**, which undermines accuracy for many forecasting methods as discussed in detail [here](https://cs.nyu.edu/~mohri/talks/NIPSTutorial2016.pdf)\n",
    "\n",
    "Next we zoom in on a smaller section of the data to get a better feel for the day-to-day profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_df = full_df[(full_df[\"timestamp\"] >= \"2012-08-01\") & (full_df[\"timestamp\"] < \"2012-09-01\")]\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "period_df.plot(x=\"timestamp\", y=\"registered_demand\", ax=ax, label=\"Registered Customers\")\n",
    "period_df.plot(x=\"timestamp\", y=\"casual_demand\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - August 2012\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see clearly the impact of day-of-week and time-of-day on demand, and note in particular that (in this summer month at least) casual riders make up a much higher proportion of trips at the weekends than in the week.\n",
    "\n",
    "At these shorter timescales, our data looks a bit more stationary and like we could extract some more robust forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/test split for eodel evaluation<a class=\"anchor\" id=\"split\"/>\n",
    "\n",
    "Since a lot of feature engineering has been done for us already, the main outstanding question is how we'll evaluate the accuracy of our models.\n",
    "\n",
    "### How to evaluate forecast models\n",
    "\n",
    "The key difference between evaluating forecasting algorithms and standard ML applications is **causality**: We must fit our model based on past data; evaluate it based on unseen \"future\" data; and make sure no future data is visible to the model fitting process.\n",
    "\n",
    "**[Backtesting](https://en.wikipedia.org/wiki/Backtesting)** (or \"hindcasting\") is the process by which we'll perform this causal evaluation: Picking one (or maybe more) historical time points to use as a cutoff scenario; training the model on data leading up to that point; and evaluating the model on data following it.\n",
    "\n",
    "<img src=\"BlogImages/backtest.png\"/>\n",
    "\n",
    "In situations (like this) where we have multiple timeseries, we draw a distinction between:\n",
    "\n",
    "* **target timeseries** (the things we want the model to forecast), and\n",
    "* **[related timeseries](https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html)** (which we know ahead of time for our forecast window)\n",
    "\n",
    "Therefore our train/test split will apply to the target timeseries only.\n",
    "\n",
    "<img src=\"BlogImages/rts_viz.png\">\n",
    "\n",
    "\n",
    "### Choosing the Setup for our Bike Forecasting example\n",
    "\n",
    "Our source data is at **hourly granularity** (sample frequency), and therefore this is the lower limit on what granularity we can sensibly forecast.\n",
    "\n",
    "Whether we choose to use this granularity or aggregate up will guide what **forecast horizon** is appropriate:\n",
    "\n",
    "* Model training will optimize overall accuracy for the given forecast horizon; so it should be set to the most useful horizon for the **business problem** (e.g. if I train a model to produce best-effort 1yr forecasts, but am actually only using it to plan 1month ahead in time, I may be missing out on performance)\n",
    "* Because we'll be comparing recurrent neural models like DeepAR, we need to consider that some of these architectures work worse when the granularity is at least a certain fraction of the forecast window: The limit to what cycle-times of patterns can be learned are proportional to the sample frequency. Amazon Forecast in particular enforces a [limit](https://docs.aws.amazon.com/forecast/latest/dg/limits.html) of max 500 samples forecast horizon\n",
    "* As discussed above, our algorithms will perform best when the data within our context (and horizon) window is approximately **stationary** and captures several cycles of the longest-period fluctuations - so quarterly or annual forecasts are probably not appropriate for the available data-set.\n",
    "\n",
    "We'll train our predictors for a **14-day (2 week) horizon**, which:\n",
    "\n",
    "* Roughly balances between the period over which half-decent weather forecasts can be made (in some countries at least); and the periods over which reasonable forward planning steps could be taken to deploy/move bikes to meet demand.\n",
    "* Is short enough to let us keep our hourly sample and not have to mess around aggregating up\n",
    "\n",
    "On the assumption (see overall volume graphs) that the end-of-year period is perhaps the *least* important time for the business, we'll further offset the final test cutoff back to 2012-12-01: Helping us avoid Christmas effects in our final evaluation window, but maximise the model's exposure to annual seasonality since only 2 years' data are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test_start = \"2012-12-01\"\n",
    "\n",
    "full_headers = full_df.columns.to_list()\n",
    "timestamp_header = \"timestamp\"\n",
    "target_headers = list(filter(lambda s: s.endswith(target_suffix), full_headers))\n",
    "target_nontotal_headers = list(filter(lambda s: s != \"total_demand\", target_headers))\n",
    "related_headers = list(filter(lambda s: (s not in target_headers) and (s != timestamp_header), full_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivoted dataframe of target variables, still sorted by timestamp:\n",
    "target_full_df = full_df[[timestamp_header] + target_nontotal_headers].melt(\n",
    "    id_vars=[timestamp_header],\n",
    "    value_vars=target_nontotal_headers,\n",
    "    var_name=\"customer_type\",\n",
    "    value_name=\"demand\"\n",
    ").sort_values(by=[timestamp_header, \"customer_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Strip \"_demand\" from the target IDs:\n",
    "target_full_df[\"customer_type\"] = target_full_df[\"customer_type\"].apply(lambda s: s[0:-len(target_suffix)])\n",
    "\n",
    "target_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_df = target_full_df[target_full_df[timestamp_header] < cutoff_test_start]\n",
    "target_test_df = target_full_df[target_full_df[timestamp_header] >= cutoff_test_start]\n",
    "\n",
    "related_df = full_df[[timestamp_header] + related_headers]\n",
    "\n",
    "print(f\"{len(target_train_df)} target training points\")\n",
    "print(f\"{len(target_test_df)} target test points\")\n",
    "print(f\"{len(related_df)} related timeseries points\")\n",
    "\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing dataframes to file...\")\n",
    "target_train_df.to_csv(\n",
    "    f\"./data/{target_train_filename}\",\n",
    "    index=False\n",
    ")\n",
    "target_test_df.to_csv(\n",
    "    f\"./data/{target_test_filename}\",\n",
    "    index=False\n",
    ")\n",
    "related_df.to_csv(\n",
    "    f\"./data/{related_filename}\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Uploading dataframes to S3...\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_train_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_train_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_test_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_test_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{related_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{related_filename}\"\n",
    ")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is uploaded to S3, we can train models and compare their performance! Move on to the next notebook to start fitting predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
